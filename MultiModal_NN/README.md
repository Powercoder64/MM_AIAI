# Multimodal_NN â€” Video + Transcript Fusion ğŸ§©
Classroomâ€‘activity detection from **preâ€‘extracted I3D features** *and* raw discourse transcripts.

> This module lives in **`MM_AIAI/Multimodal_NN/`**  

---

## ğŸ“‘ Table of Contents
1. [Features](#features)  
2. [Quick Start](#quick-start)  
3. [Inputs](#inputs)  
4. [Outputs](#outputs)  
5. [Project Structure](#project-structure)  
6. [Configuration](#configuration)  
7. [Dependencies](#dependencies)  
8. [Pre-Trained Weights](#pre-trained-weights)

---

<a id="features"></a>
## âœ¨ Features
* **Oneâ€‘liner inference**

  ```bash
  python Neural_Network.py --filename=my_clip.mp4
  ```

* **Multimodal fusion** of  
  â€¢ RGB embeddingsâ€ƒ+â€ƒFlow embeddingsâ€ƒ+â€ƒ20â€‘class transcript oneâ€‘hots  
* **CSV matrix output** with **24 activity classes** (rows) Ã— **seconds** (columns)  
* Docker container & Conda recipe included  
* All data stays inside **`./data/`** (override via `options.py`)

---

<a id="quick-start"></a>
## ğŸš€ Quick Start

### 1 Â· Clone the parent project
```bash
git clone https://github.com/Powercoder64/MM_AIAI.git
cd MM_AIAI/Multimodal_NN
```

### 2 Â· Build the Docker image
```bash
docker build -t multimodal-nn -f Dockerfile .
```

### 3 Â· Run the model inside Docker
```bash
# 1) drop your files in the right subâ€‘folders (see below)
# 2) launch the container
docker run --gpus all                                              -v $(pwd)/data:/app/data                                 -v $(pwd)/models:/app/models                             multimodal-nn                                            python Neural_Network.py --filename=my_clip.mp4
```

### 4 Â· Local run (conda / pip)
```bash
conda env create -f conda_env_mm_nn.yml
conda activate mm_nn

python Neural_Network.py --filename=my_clip.mp4
```

---

<a id="inputs"></a>
## ğŸ“¥ Inputs

```text
data/
â”œâ”€â”€ video/              # raw classroom clips               (*.mp4)
â”‚   â””â”€â”€ my_clip.mp4
â”œâ”€â”€ features/           # I3D outputs from Pre_Processing
â”‚   â”œâ”€â”€ my_clip_rgb.npy
â”‚   â””â”€â”€ my_clip_flow.npy
â””â”€â”€ transcripts/        # annotated discourse spreadsheets
    â””â”€â”€ my_clip.xlsx    # sheet â€œCoding Labelsâ€
```
*The default root (`./data`) can be changed in **`options.py`**.*

---

<a id="outputs"></a>
## ğŸ“¦ Outputs

After a run you will find:

| File (for `my_clip`) | Shape / Type | Description |
|----------------------|--------------|-------------|
| `my_clip_MATRIX.csv` | 24 Ã— *S*     | Binary **activity matrix**<br>rows = classes, cols = seconds |

<details>
<summary>CSV preview (first 5â€…s, 4 classes)</summary>

```text
,0001,0002,0003,0004,0005
Whole_Class_Activity,0,1,1,1,0
Individual_Activity ,0,0,1,0,0
Small_Group_Activity,0,0,0,0,0
Book-Using_or_Holding,0,0,1,1,1
...
```
*0 = class absent, 1 = class present (per second).*
</details>

---

<a id="project-structure"></a>
## ğŸ—‚ Project Structure

```text
Multimodal_NN/
â”œâ”€â”€ Neural_Network.py          # entry point (arg --filename)
â”œâ”€â”€ options.py                 # paths & hyperâ€‘params
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ BaSNet_model_best.pkl  # â‡¦ download & place here
â”‚   â””â”€â”€ MM_Transformer.pth     # â‡¦ provided in repo
â”œâ”€â”€ data/                      # default data root
â”‚   â”œâ”€â”€ video/
â”‚   â”œâ”€â”€ features/
â”‚   â””â”€â”€ transcripts/
â”œâ”€â”€ utils/                     # data loaders, metrics, logging
â”œâ”€â”€ Dockerfile
â””â”€â”€ conda_env_mm_nn.yml
```

---

<a id="configuration"></a>
## âš™ï¸ Configuration

| Parameter        | Default      | Where to change                                     |
|------------------|--------------|-----------------------------------------------------|
| Data root        | `./data/`    | `options.py â†’ DATA_PATH`                            |
| Output directory | `./outputs/` | `options.py â†’ OUT_DIR`                              |
| Device           | CUDA if available | `--device cpu|cuda`                          |
| Checkpoint path  | `models/MM_Transformer.pth` | fusion network (default)            |

---

<a id="dependencies"></a>
## ğŸ“š Dependencies
*(full list in Dockerfile & Conda YAML)*

| Package                 | Purpose                                  |
|-------------------------|------------------------------------------|
| **PyTorch â‰¥ 2.2 + CUDA**| Backbone & fusion network                |
| **numpy**, **pandas**   | Feature I/O, CSV/XLSX handling           |
| **openpyxl**            | Read annotated Excel transcripts         |
| **scikitâ€‘learn**        | F1 / metric utilities                    |
| **opencvâ€‘python**       | Video post-processing     |

---

<a id="pre-trained-weights"></a>
## ğŸ“¥ Pre-Trained Weights

| File | Purpose |
|------|---------|
| **MM_Transformer.pth** | Multimodal fusion checkpoint. **Already included** in `models/`; no download needed. |
| **BaSNet_model_best.pkl** | Video baseline model. Download and place in `models/`; can also be tested for video baseline. |

Download BaSNet:

```bash
wget -O BaSNet_model_best.pkl   'https://drive.google.com/uc?export=download&id=1d0qPeMQSjOrllvrjKdMqkhC5gf0hEREt'

mkdir -p models
mv BaSNet_model_best.pkl models/
```

Resulting tree:
```text
models/
â”œâ”€â”€ BaSNet_model_best.pkl
â””â”€â”€ MM_Transformer.pth
```

---

### â–¶ï¸ Full example
```bash
python Neural_Network.py --filename=my_clip.mp4
# â†’  outputs/my_clip_MATRIX.csv
```

Enjoy secondâ€‘wise classroomâ€‘activity matrices for your downstream analyses! ğŸ‰
